{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\shulin\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.752 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3089\n",
      "首先 说 一下 物流 ， 中午 十一点左右 下单 ， 下午 5 点多 就 到 了 ， 速度 真的 是 快 。 在 买 之前 ， 纠结 了 好久 ， 不 知道 到底 买多大 的 ， 最后 决定 买 32G 的 ， 毕竟 只是 作为 娱乐 工具 ， 够用 就 好 了 。 买回来 之后 用 了 几天 才 来 评价 ， 说实话 真的 是 很 好 。 反应速度 非常 快 ， 没有 卡顿 现象 ， 至于 屏幕 ， 个人感觉 并 没有 网上 的 那些 问题 ， 色泽 清晰 ， 鲜明 。 \n",
      " 1\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "review_data=[]\n",
    "labels=[]\n",
    "f_pos=open('./data/pos.txt','r',encoding='utf8')\n",
    "for line in f_pos:\n",
    "    if line.strip('\\n')!=None:\n",
    "        tokens=list(jieba.cut(line))\n",
    "        processed_sent=\" \".join(tokens)\n",
    "        review_data.append(processed_sent)\n",
    "        labels.append(1)\n",
    "\n",
    "f_neg=open('./data/neg.txt','r',encoding='utf8')\n",
    "for line in f_neg:\n",
    "    if line.strip('\\n')!=None:\n",
    "        tokens=list(jieba.cut(line))\n",
    "        processed_sent=\" \".join(tokens)\n",
    "        review_data.append(processed_sent)\n",
    "        labels.append(0)\n",
    "print(len(review_data))\n",
    "print(review_data[3],labels[3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_len: 61.081579799287795\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "avglen = 0 #句子最大长度\n",
    "len_list=[]\n",
    "for sent_str in review_data:\n",
    "    words=list(jieba.cut(sent_str))\n",
    "    length = len(words)\n",
    "    len_list.append(length)\n",
    "avglen=np.sum(np.array(len_list))/len(len_list)\n",
    "print('avg_len:',avglen)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 14, 20, 1, 9, 365, 2, 123, 1, 1044, 94, 234, 2, 3520, 43, 100, 40, 305, 10, 2495, 3521, 2, 388, 1, 1138, 27, 377, 110, 3522, 1045, 1, 779, 267, 780, 2, 20, 23, 1, 1657, 38, 78, 23, 40, 2, 234, 7, 474, 31, 1, 6, 50, 29, 23, 781, 3523, 295, 1, 228, 3524, 2, 1, 7, 8, 3, 50, 337, 2, 1, 601, 1436, 15, 3525, 966, 1, 69, 2496, 1045, 329, 446, 2, 1, 1276, 29, 1437, 2497, 421, 1045, 43, 9, 66, 3526, 397, 174, 1658, 229, 8, 548, 2, 1, 149, 6, 7, 156, 3, 632, 295, 74, 3527, 7, 330, 1659, 2019, 107, 1660, 3, 4]\n",
      "[  25   14   20    1    9  365    2  123    1 1044   94  234    2 3520\n",
      "   43  100   40  305   10 2495 3521    2  388    1 1138   27  377  110\n",
      " 3522 1045    1  779  267  780    2   20   23    1 1657   38   78   23\n",
      "   40    2  234    7  474   31    1    6   50   29   23  781 3523  295\n",
      "    1  228 3524    2    1    7    8    3   50  337    2    1  601 1436\n",
      "   15 3525  966    1   69 2496 1045  329  446    2    1 1276   29 1437\n",
      " 2497  421 1045   43    9   66 3526  397  174 1658  229    8  548    2\n",
      "    1  149]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "tokenizer = Tokenizer() # 创建一个Tokenizer对象，将一个词转换为正整数\n",
    "tokenizer.fit_on_texts(review_data) #将词编号，词频越大，编号越小\n",
    "word2index = tokenizer.word_index\n",
    "vocab_size=len(word2index)\n",
    "#print(vocab,len(vocab))\n",
    "index2word = {word2index[word]:word for word in word2index}\n",
    "x_word_ids = tokenizer.texts_to_sequences(review_data) #将句子中的每个词转换为数字\n",
    "print(x_word_ids[1])\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "x_padded_seqs = pad_sequences(x_word_ids,truncating='post',maxlen=100)#将每个句子设置为\n",
    "x_padded_seqs=np.array(x_padded_seqs)\n",
    "print(x_padded_seqs[1])\n",
    "#print(vocab)\n",
    "#print(x_padded_seqs[2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense,Activation,Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "# 创建深度学习模型， Embedding + LSTM + Softmax.\n",
    "def create_LSTM(n_units, input_size, output_dim,vocab_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size + 1, output_dim=output_dim,\n",
    "    input_length=input_size, mask_zero=True))\n",
    "    model.add(LSTM(n_units, input_shape=(None,input_size)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(output_dim , activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 2)            14480     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               41200     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 55,882\n",
      "Trainable params: 55,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "78/78 [==============================] - 7s 53ms/step - loss: 0.5649 - accuracy: 0.6912\n",
      "Epoch 2/5\n",
      "78/78 [==============================] - 3s 42ms/step - loss: 0.5269 - accuracy: 0.7718\n",
      "Epoch 3/5\n",
      "78/78 [==============================] - 3s 43ms/step - loss: 0.3726 - accuracy: 0.8834\n",
      "Epoch 4/5\n",
      "78/78 [==============================] - 3s 44ms/step - loss: 0.2135 - accuracy: 0.9425\n",
      "Epoch 5/5\n",
      "78/78 [==============================] - 4s 49ms/step - loss: 0.1852 - accuracy: 0.9478\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x23573235ef0>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x_padded_seqs,labels,test_size=0.2,shuffle=0)\n",
    "x_train=np.array(x_train)\n",
    "x_test=np.array(x_test)\n",
    "y_train=np.array(y_train)\n",
    "y_test=np.array(y_test)\n",
    "y_train_onehot=to_categorical(y_train)\n",
    "y_test_hot=to_categorical(y_test)\n",
    "# 模型输入参数，需要自己根据需要调整\n",
    "input_size = x_padded_seqs.shape[1]\n",
    "print(input_size)\n",
    "n_units = 100\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "output_dim = 2\n",
    "# 模型训练\n",
    "lstm_model = create_LSTM(n_units, input_size, output_dim,vocab_size=vocab_size)\n",
    "lstm_model.fit(x_train, y_train_onehot, epochs=epochs, batch_size=batch_size,verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率 0.9239482200647249\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "results=lstm_model.predict(x_test)\n",
    "result_labels = np.argmax(results, axis=-1) # 获得最大概率对应的标签\n",
    "#print(result_labels)\n",
    "print('准确率', accuracy_score(y_test, result_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[241, 94, 213, 7, 124]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0 241  94 213   7 124]]\n"
     ]
    }
   ],
   "source": [
    "new_reviews=['体验不是很好，信号差还发烫。手机第一次充电就发烫的要死。热点总是自动打开']\n",
    "new_sents=[]\n",
    "for sent_str in new_reviews:\n",
    "    tokens=jieba.cut(sent_str)\n",
    "    sent=' '.join(tokens)\n",
    "    new_sents.append(sent)\n",
    "x_new_ids = tokenizer.texts_to_sequences(new_sents) #将句子中的每个词转换为数字\n",
    "print(x_new_ids[0])\n",
    "x_new_padseqs = pad_sequences(x_new_ids,truncating='post',maxlen=100)#将每个句子设置为\n",
    "print(x_new_padseqs)\n",
    "probs=lstm_model.predict(x_new_padseqs)\n",
    "new_labels=np.argmax(probs,axis=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(new_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}