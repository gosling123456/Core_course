{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24a78e6e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "基于TF-IDF的文本分类和聚类\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e29e61",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "TF-IDF文本向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae77486a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "text=[]\n",
    "\n",
    "def read_files(path):\n",
    "    f_read1=open(path,encoding='utf8',errors='ignore')\n",
    "    for line in f_read1:\n",
    "        line=line.replace('\\\\u0009','').replace('\\\\n','')\n",
    "        obj=json.loads(line)\n",
    "        sent=obj['contentClean']\n",
    "        text.append(sent)\n",
    "\n",
    "read_files('./nlp_data1/体育.json')\n",
    "read_files('./nlp_data1/从政.json')\n",
    "read_files('./nlp_data1/国际.json')\n",
    "read_files('./nlp_data1/经济.json')\n",
    "#print(text)\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31a0eacb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['02', '08', '10', '11', '12', '13', '14', '15', '18', '2014年', '2015年', '21世纪经济报道', '一带一路', '三严三实', '上海', '不过', '与此同时', '两学一做', '中国', '中国杯', '为此', '也就是说', '事实上', '于是', '互联网', '今天', '今年以来', '他表示', '他说', '伊斯兰国', '但是', '作者', '例如', '值得一提的是', '值得注意的是', '党组', '其中', '其实', '其次', '北京', '十三五', '南方都市报', '双11', '另一方面', '另外', '同时', '四风', '因为', '因此', '她说', '如今', '实际上', '对此', '左右', '带病提拔', '广州', '广马', '当前', '当时', '当然', '意见', '慰安妇', '所以', '报道', '报道称', '据了解', '据悉', '据报道', '据统计', '数据显示', '日前', '日本', '时代', '来源', '根据', '此前', '此外', '比如', '然而', '现在', '目前', '第一', '第三', '第二', '等等', '美国', '英国', '萨德', '规定', '记者', '译文', '近年来', '近日', '那么', '释义', '针对', '问题', '除此之外', '随后', '首先']\n",
      "\n",
      "\n",
      "tf-idf向量化的结果为：\n",
      " [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.32615797 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "2000\n",
      "(2000, 100)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "corpus = text\n",
    "# corpus = [\n",
    "#             'This is the first document.',\n",
    "#             'This document is the second document.',\n",
    "#             'Beijing is our capital.',\n",
    "#             'Is this the first document?',\n",
    "#             'Beijing is the capital of China.',\n",
    "# ]\n",
    "# #建立一个对象\n",
    "vectorizer = TfidfVectorizer(max_features=100)\n",
    "\n",
    "\n",
    "#从文本集合到语义向量\n",
    "#将文档转换为文档 - 词矩阵。返回稀疏矩阵\n",
    "#向量化的表示\n",
    "#toArray()方法是List接口中提供的方法，用来实现List对象转换为数组对象的功能。\n",
    "#把格式转换为numpy的数组格式\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "\n",
    "#获得所含的一些词\n",
    "print(vectorizer. get_feature_names())\n",
    "\n",
    "print('\\n')\n",
    "print('tf-idf向量化的结果为：\\n', X)\n",
    "print(X[0])\n",
    "\n",
    "\n",
    "k = len(text)\n",
    "print(k)\n",
    "Y= np.random.randint(low=0,high=4,size=(k),dtype='int')\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ae3623",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "K-折交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4d6e733",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 100) (400, 100) (1600,) (400,)\n",
      "[403, 407, 390, 400]\n",
      "1600 400 1600 400\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=33, test_size=0.2)\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "label=[0,0,0,0]\n",
    "for i in y_train.tolist():\n",
    "    if i==0:\n",
    "        label[0] += 1\n",
    "    if i==1:\n",
    "        label[1] += 1\n",
    "    if i==2:\n",
    "        label[2] += 1\n",
    "    if i==3:\n",
    "        label[3] += 1\n",
    "\n",
    "print(label)\n",
    "\n",
    "print(len(x_train),len(x_test),len(y_train),len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6d2f1a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "TF-IDE 文本分类 支持向量机SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5f6f4d2",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[1]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Y=np.array([0,0,1,0,1])\n",
    "# # #print('对应的类别标签为',Y)\n",
    "\n",
    "\n",
    "# print(X.shape)\n",
    "# print(Y.shape)\n",
    "\n",
    "\n",
    "from sklearn import svm\n",
    "#svm 支持向量机\n",
    "\n",
    "#最后一个做测试，其他的训练\n",
    "X_train=X[:X.shape[0]-1]\n",
    "#print(X.shape[0]-1)\n",
    "Y_train=Y[:Y.shape[0]-1]\n",
    "#'Beijing is the capital of China.',\n",
    "\n",
    "\n",
    "#最后一个做测试\n",
    "#'Beijing is the capital of China.',\n",
    "X_test=X[X.shape[0]-1:]\n",
    "#'Beijing is the capital of China.',\n",
    "Y_test=Y[Y.shape[0]-1:]\n",
    "\n",
    "\n",
    "#建立一个分类器对象，线性核函数，概率默认是显示的\n",
    "clf =svm.SVC(kernel='linear', probability=True)\n",
    "#训练好的分类器通过fit去训练分类器\n",
    "clf.fit(X_train,Y_train)\n",
    "\n",
    "\n",
    "#评估模型的得分\n",
    "score=clf.score(X_test,Y_test)\n",
    "print(score)\n",
    "\n",
    "#用分类器predict去预测测试集，预测出其标签为1\n",
    "print(clf.predict(X_test))\n",
    "print(type(clf.predict(X_test)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357ff6dd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "TF-IDE   sklearn实现KNN分类算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad0a3612",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 100)\n",
      "(2000,)\n",
      "[1 0 0 0 0 3 1 2 0 3 3 2 0 0 3 0 0 0 0 2 0 0 0 0 0 1 0 0 1 3 0 2 2 0 1 0 0\n",
      " 0 0 0 1 3 0 0 0 2 3 0 0 2 0 2 1 0 1 2 2 0 0 2 2 0 0 0 0 3 3 0 0 2 1 0 0 1\n",
      " 1 0 1 3 0 0 1 3 0 0 0 2 0 0 0 0 2 1 2 2 2 0 0 2 1 0 2 0 0 3 0 0 0 0 0 0 2\n",
      " 1 0 0 0 1 0 0 0 0 3 0 0 1 0 3 3 0 2 0 0 3 2 3 2 3 2 0 0 0 0 0 0 0 0 2 0 1\n",
      " 0 0 0 0 0 0 2 0 2 1 0 0 3 0 0 0 0 0 2 3 1 1 2 0 0 3 2 0 0 0 0 0 0 0 2 1 0\n",
      " 0 0 0 3 0 0 0 2 0 0 0 1 0 0 0 0 2 0 1 0 3 1 0 3 0 0 2 1 0 1 0 1 0 3 0 0 0\n",
      " 0 0 0 1 0 2 0 1 0 0 2 2 2 0 2 1 1 3 0 0 0 1 0 0 3 1 1 0 2 0 0 1 1 0 0 3 2\n",
      " 1 1 0 2 0 1 1 3 0 2 1 0 0 1 3 3 0 0 0 2 1 0 0 2 2 3 1 0 0 0 2 3 0 0 1 1 0\n",
      " 0 0 1 1 0 0 2 0 0 2 3 0 0 2 2 2 0 1 2 0 0 3 0 0 0 3 0 2 3 3 2 2 2 3 2 1 0\n",
      " 1 1 0 3 0 1 0 0 0 2 0 3 0 0 1 0 0 2 0 3 0 0 0 0 0 0 0 2 2 0 3 3 1 2 0 0 2\n",
      " 1 2 0 0 2 2 0 0 2 0 0 0 1 2 0 0 2 0 1 0 1 2 0 0 0 1 3 0 0 0]\n",
      "<class 'numpy.ndarray'>\n",
      "0.3835\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "\n",
    "#++++++++++++++++++++++\n",
    "#构建knn分类模型，并指定 k 值\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "KNN=KNeighborsClassifier(n_neighbors=4)\n",
    "#使用训练集训练模型\n",
    "KNN.fit(x_train,y_train)\n",
    "\n",
    "# #评估模型的得分\n",
    "# score=KNN.score(x_test)\n",
    "# print(score)\n",
    "\n",
    "print(KNN.predict(x_test))\n",
    "print(type(KNN.predict(x_test)))\n",
    "# print(X)\n",
    "\n",
    "\n",
    "\n",
    "#评估模型的得分\n",
    "\n",
    "score=KNN.score(X,Y)\n",
    "\n",
    "print(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df9ee3a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "朴素贝叶斯算法应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac274aa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#+++++++++++++++++++++\n",
    "# #朴素贝叶斯\n",
    "#\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "#\n",
    "# bayes_modle=GaussianNB()\n",
    "# #训练数据\n",
    "# bayes_modle.fit(x_train,y_train)\n",
    "#\n",
    "#\n",
    "# #评估模型的得分,比较一下\n",
    "# score=KNN.score(X,Y)\n",
    "# print(score)\n",
    "#\n",
    "# #使用模型进行分类预测\n",
    "# result=bayes_modle.predict(x_test)\n",
    "# print(result)\n",
    "#\n",
    "# model_score=bayes_modle.score(X,Y)\n",
    "# print(model_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27c6319",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "上述为分类，下述为聚类\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127333a6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "基于TF-IDF的文本聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6c8fcd",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ab1c59",
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# #已经知道一一向量，直接调用kmeans\n",
    "# import matplotlib.pyplot as plt\n",
    "#\n",
    "#\n",
    "# from sklearn.cluster import KMeans\n",
    "#\n",
    "#\n",
    "# #首先，建立一个对象，然后通过fit，在训练集上进行训练\n",
    "# #n_clusters=2 表示把整个数据集聚成两类\n",
    "# kmeans = KMeans(n_clusters=4)\n",
    "# kmeans.fit(X)\n",
    "# result1 = kmeans.predict(X)\n",
    "# #对原始样本X进行预测\n",
    "# print(kmeans.predict(X))\n",
    "# #五个文档的标签分别是[0 0 1 0 1]\n",
    "#\n",
    "# #print(kmeans.cluster_centers_)\n",
    "# #已经知道有两个聚类，一个是“0”，一个是“1”，聚类中心，通过中心，可以知道两个向量的距离\n",
    "# plt.scatter(X[:, 0], X[:, 1], c=result1)\n",
    "#\n",
    "# plt.show()\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b573017f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "基于TF-IDF的文本聚类​ DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799dd87f",
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.cluster import DBSCAN\n",
    "#\n",
    "#\n",
    "#\n",
    "# dbscan=DBSCAN()\n",
    "#\n",
    "# print(X.shape)\n",
    "#\n",
    "# #对原始样本X进行预测\n",
    "# result2 = dbscan.fit_predict(X)\n",
    "# #五个文档的标签分别是[0 0 1 0 1]\n",
    "# print(result2)\n",
    "# print(result.shape)\n",
    "# #print(kmeans.cluster_centers_)\n",
    "# #已经知道有两个聚类，一个是“0”，一个是“1”，聚类中心，通过中心，可以知道两个向量的距离\n",
    "# plt.scatter(X[:, 0], X[:, 1], c=result2)\n",
    "#\n",
    "# plt.show()\n",
    "#\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463fa9dd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "基于doc2vec的文本分类和聚类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9496032e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "读取训练数据，并将读取的句子存储到text中（同\n",
    "chapter 7代码）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720584c3",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "text=[]\n",
    "f_read=open('./体育.json', 'r',encoding='utf8',errors='ignore')\n",
    "for line in f_read:\n",
    "    line=line.replace('\\\\u0009','').replace('\\\\n','')\n",
    "    \n",
    "    #把每一行加载成一个json对象\n",
    "    obj=json.loads(line)\n",
    "    \n",
    "    #通过json对象的key值，选出我们需要的文本\n",
    "    sent=obj['contentClean']\n",
    "    \n",
    "    #把文本放到列表中\n",
    "    text.append(sent)\n",
    "print(len(text))\n",
    "print(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79f0de9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "processed_text=[]\n",
    "for sent in text:\n",
    "    #完成了两步\n",
    "    #对文本之中的每个句子进行jieba分词，分词的结果放在processed_tex中\n",
    "    processed_sent=jieba.cut(sent.strip(' '))\n",
    "    processed_text.append(list(processed_sent))\n",
    "print(processed_text[0])\n",
    "\n",
    "#是上述就是预处理了"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b563986a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "基于doc2vec的文本向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c75a33",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# import gensim\n",
    "# from gensim.models.doc2vec import Doc2Vec\n",
    "# train_text=[]\n",
    "# print(len(processed_text))\n",
    "# for i,sent in enumerate(processed_text):\n",
    "#     #改变成Doc2vec所需要的输入样本格式，\n",
    "#     #由于gensim里Doc2vec模型需要的输入为固定格式，输入样本为：[句子，句子序号],这里需要\n",
    "#     tagged_doc=gensim.models.doc2vec.TaggedDocument(sent,tags=[i])\n",
    "#     train_text.append(tagged_doc)\n",
    "#\n",
    "# d_model = Doc2Vec(train_text,min_count=1,window=3,vector_size=300,sample=1e-3,negative=5,workers=4)\n",
    "# d_model.train(train_text,total_examples=d_model.corpus_count,epochs=10)\n",
    "# d_model.save(\"doc2vec_model\") #保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e138bc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# import gensim\n",
    "# from gensim.models.doc2vec import Doc2Vec\n",
    "# #load doc2vec model...\n",
    "# d_model= gensim.models.doc2vec.Doc2Vec.load(\"doc2vec_model\")\n",
    "# #load train vectors...\n",
    "# #得到模型中保存的每个句子的语义向量\n",
    "# text_vecs= d_model.docvecs.vectors_docs\n",
    "# print(\"专利向量的个数为\",len(text_vecs))\n",
    "# print(text_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5934580",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "基于doc2vec的文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e472e330",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "# from gensim.models.doc2vec import Doc2Vec\n",
    "# #load doc2vec model...\n",
    "d_model= gensim.models.doc2vec.Doc2Vec.load(\"doc2vec_model\")\n",
    "#load train vectors...\n",
    "#得到模型中保存的每个句子的语义向量  \n",
    "text_vecs= d_model.docvecs.vectors_docs\n",
    "print(\"专利向量的个数为\",len(text_vecs))\n",
    "print(X)\n",
    "X = text_vecs\n",
    "print(X is text_vecs)\n",
    "#print(text_vecs[0])\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2353c889",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "K-折交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99689aa4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=33, test_size=0.2)\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "print(len(x_train),len(x_test),len(y_train),len(y_test))\n",
    "print(len(x_train),len(x_test))\n",
    "label=[0,0,0,0]\n",
    "# for i in y_train.tolist():\n",
    "#     if i==0:\n",
    "#         label[0] += 1\n",
    "#     if i==1:\n",
    "#         label[1] += 1\n",
    "#     if i==2:\n",
    "#         label[2] += 1\n",
    "#     if i==3:\n",
    "#         label[3] += 1\n",
    "# print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267e6a04",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "朴素贝叶斯算法应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f0b9be",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "bayes_modle=GaussianNB()\n",
    "#训练数据\n",
    "bayes_modle.fit(x_train,y_train)\n",
    "#使用模型进行分类预测\n",
    "result=bayes_modle.predict(x_test)\n",
    "# print(result)\n",
    "# print(X.shape)\n",
    "# print(Y.shape)\n",
    "print(X,'\\n')\n",
    "#评估模型的得分\n",
    "# score=KNN.score(X,Y)\n",
    "# print(score)\n",
    "\n",
    "model_score=bayes_modle.score(X,Y)\n",
    "print(model_score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d9d0a9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "sklearn实现KNN分类算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89ec2f9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#构建knn分类模型，并指定 k 值\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "KNN=KNeighborsClassifier(n_neighbors=4)\n",
    "#使用训练集训练模型\n",
    "KNN.fit(x_train,y_train)\n",
    "\n",
    "# #评估模型的得分\n",
    "# score=KNN.score(x_test)\n",
    "# print(score)\n",
    "\n",
    "print(KNN.predict(x_test))\n",
    "print(type(KNN.predict(x_test)))\n",
    "# print(X)\n",
    "\n",
    "\n",
    "\n",
    "#评估模型的得分\n",
    "\n",
    "score=KNN.score(X,Y)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc24025b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "基于doc2vec的文本聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfef4846",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# kmeans = KMeans(n_clusters=3).fit(text_vecs)\n",
    "# print(kmeans.predict(text_vecs))\n",
    "\n",
    "# k-means方法聚类\n",
    "model = KMeans(n_clusters=4)\n",
    "model.fit(text_vecs)\n",
    "result = model.predict(text_vecs)\n",
    "print(result)\n",
    "plt.scatter(text_vecs[:, 0], text_vecs[:, 1], c=result)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_vecs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-11-c93c9f5b8d8e>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mDBSCAN\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0meps\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmin_samples\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m50\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;31m# model = DBSCAN()\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext_vecs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit_predict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext_vecs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'text_vecs' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# DBSCAN方法聚类\n",
    "model = DBSCAN(eps=0.2, min_samples=50)\n",
    "# model = DBSCAN()\n",
    "model.fit(text_vecs)\n",
    "result = model.fit_predict(text_vecs)\n",
    "print(result)\n",
    "plt.figure()\n",
    "plt.scatter(text_vecs[:, 0],text_vecs[:, 1], c=result)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(X_test.shape[0]):\n",
    "    if clf.predict(X_train[i:i + 1, :]) == Y_test[i]:\n",
    "        correct += 1\n",
    "print(correct/len(Y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}